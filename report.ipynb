{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# ðŸ‡ºðŸ‡¦ ZNO Solver: How I survived \"Offline Mode\"\n",
        "\n",
        "**Task:** Solve Ukrainian ZNO exams.\n",
        "**Constraint:** No Internet. No API calls.\n",
        "**My Hardware:** A Kaggle GPU that cries if you look at it wrong.\n",
        "\n",
        "Basically, I couldn't just call GPT-4o. I had to make a small, dumb model (Qwen 7B) act like a smart model, all while cut off from the outside world.\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 1: The \"Professor\" (Cheating with Gemini)\n",
        "\n",
        "Qwen 2.5 7B is decent, but on Ukrainian History or Literature, it mostly just guesses \"B\" and prays. I needed it to actually *think*.\n",
        "\n",
        "Since I could use the internet before the submission, I used **Gemini 3.0 Flash Preview** to generate a \"Reasoning Dataset.\" I fed it 3k raw questions and forced it to explain why an answer is right.\n",
        "\n",
        "**The Prompt I used:**\n",
        "\n",
        "```python\n",
        "prompt = f\"\"\"\n",
        "You are a ZNO tutor.\n",
        "Question: {q}\n",
        "Options: {opts}\n",
        "Task: Explain the logic in 1-2 sentences. End with \"Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ: [Letter]\".\n",
        "\"\"\"\n",
        "\n",
        "```\n",
        "\n",
        "**Result:** I got a dataset where the AI explains that *\"Panas Myrny wrote 'Hiba revut voly...', so the answer is A\"* instead of just saying *\"A\"*.\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: The \"Student\" (Fine-Tuning)\n",
        "\n",
        "I took that data and force-fed it to Qwen using **QLoRA** (because I don't have enough VRAM to train the full model).\n",
        "\n",
        "**Snippet from `train.py`:**\n",
        "\n",
        "```python\n",
        "# LoRA Config because I am poor (VRAM-wise)\n",
        "peft_config = LoraConfig(\n",
        "    r=16, lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Hit everything\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "It trained for about an hour. The loss went down. I felt like a genius. (Spoiler: I was not).\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 3: The Nightmare (Inference)\n",
        "\n",
        "This is where everything went wrong.\n",
        "\n",
        "### Attempt 1: \"Just ask it nicely\" (Generation)\n",
        "\n",
        "I tried to make the model generate the explanation like I trained it to.\n",
        "\n",
        "**The Prompt:**\n",
        "`P: <Question>... Answer this.`\n",
        "**The Output:**\n",
        "`A: Analysis: Taras Shevchenko was a great artist. He painted many things. The answer is... [Model continues writing for 500 tokens about Shevchenko's life] ... Answer: B.`\n",
        "\n",
        "**The Problem:**\n",
        "\n",
        "1. It wouldn't shut up.\n",
        "2. **Hallucinations:** It would write a perfectly logical explanation for why the earth is flat and then pick the wrong letter.\n",
        "3. **Parsing Hell:** I tried Regex to find the letter. It failed 30% of the time because the model wrote \"Answer is (A)\" or \"Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ - Ð\" or \"I think A\".\n",
        "\n",
        "**Score:** Terrible.\n",
        "\n",
        "---\n",
        "\n",
        "### Attempt 2: \"Math over Vibes\" (Logit Scoring)\n",
        "\n",
        "I read a paper (`2025.unlp-1.2.pdf`) that said: *Don't let the model talk. Just look at the math.*\n",
        "\n",
        "Instead of generating text, I checked the probability of the next token being 'Ð', 'Ð‘', 'Ð’', 'Ð“', or 'Ð”'.\n",
        "\n",
        "**The Code:**\n",
        "\n",
        "```python\n",
        "# Force the model to look at the math\n",
        "with torch.inference_mode():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits[:, -1, :] # Last token\n",
        "    scores = logits[:, [id_A, id_B, id_C, id_D, id_E]]\n",
        "    winner = torch.argmax(scores)\n",
        "\n",
        "```\n",
        "\n",
        "**The Mistake:**\n",
        "I got `CUDA OutOfMemory Error` immediately.\n",
        "I tried `BATCH_SIZE = 32`. **Crash.**\n",
        "I tried `BATCH_SIZE = 8`. **Crash.**\n",
        "\n",
        "**The Fix:**\n",
        "`BATCH_SIZE = 1`. Yes, it took 2 hours to run. But it didn't crash.\n",
        "\n",
        "---\n",
        "\n",
        "### Attempt 3: The \"Bias\" Problem\n",
        "\n",
        "My logit scorer was working, but I noticed it picked \"A\" or \"B\" way too often.\n",
        "Why? Because the model is lazy.\n",
        "\n",
        "**The Solution: Few-Shot Prompting**\n",
        "I added 4 examples to the prompt before the real question.\n",
        "\n",
        "* Example 1 -> Answer A\n",
        "* Example 2 -> Answer B\n",
        "* Example 3 -> Answer G\n",
        "* Example 4 -> Answer D\n",
        "\n",
        "This forced the model to \"remember\" that other letters exist.\n",
        "\n",
        "**Final Code Snippet (The one that actually worked):**\n",
        "\n",
        "```python\n",
        "# The \"Nuclear Option\" Prompt\n",
        "FEW_SHOT = \"\"\"\n",
        "...Example 1 (Ans: A)...\n",
        "...Example 2 (Ans: B)...\n",
        "...Example 3 (Ans: G)...\n",
        "...Example 4 (Ans: D)...\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"Ð”Ð°Ð¹ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð±ÑƒÐºÐ²Ð¾ÑŽ-Ð²Ð°Ñ€Ñ–Ð°Ð½Ñ‚Ð¾Ð¼.\"\n",
        "final_prompt = f\"{instruction}\\n\\n{FEW_SHOT}\\n\\nQuestion: {q}\\nAnswer:\"\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## The \"Offline\" Struggle\n",
        "\n",
        "You don't appreciate `pip install` until it's gone.\n",
        "I had to download every library on my laptop, zip them, upload them to Kaggle, and install them like this:\n",
        "\n",
        "```python\n",
        "# I hate this part\n",
        "!pip install --no-index --find-links=offline_libs bitsandbytes peft transformers\n",
        "\n",
        "```\n",
        "\n",
        "**Fun error I got:**\n",
        "`ModuleNotFoundError: validate_bnb_backend_availability`\n",
        "*Translation:* Your bitsandbytes version hates your transformers version.\n",
        "*Fix:* Uninstalled everything, reinstalled in specific order. Cried a little.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Results\n",
        "\n",
        "* **Random Guessing:** ~20%\n",
        "* **Text-Only Baseline:** ~34%\n",
        "* **My Score:** **~38%**\n",
        "\n",
        "Is it perfect? No.\n",
        "Did I beat the baseline without using RAG or the Internet? **Yes.**\n",
        "\n",
        "We hit a \"Knowledge Ceiling.\" The model is small (7B). It just doesn't know some specific facts about Ukrainian history, and no amount of prompt engineering can fix missing neurons. To get >50%, we'd need RAG (searching a textbook), but for a pure model approach, this is the limit."
      ],
      "metadata": {
        "id": "mxoEGW4KCnjF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "171fzaQbCn9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}