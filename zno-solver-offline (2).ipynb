{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":126776,"databundleVersionId":15042800,"sourceType":"competition"},{"sourceId":14485074,"sourceType":"datasetVersion","datasetId":9251850},{"sourceId":14489737,"sourceType":"datasetVersion","datasetId":9254785},{"sourceId":166258,"sourceType":"modelInstanceVersion","modelInstanceId":141469,"modelId":164048}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# Force bitsandbytes to use the CUDA modules it finds in its own folder\nos.environ[\"BNB_CUDA_VERSION\"] = \"121\" # Or 118, depending on Kaggle's current CUDA\nos.environ[\"LD_LIBRARY_PATH\"] += \":/usr/local/cuda/lib64\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:17:38.937742Z","iopub.execute_input":"2026-01-13T21:17:38.938017Z","iopub.status.idle":"2026-01-13T21:17:38.944596Z","shell.execute_reply.started":"2026-01-13T21:17:38.937986Z","shell.execute_reply":"2026-01-13T21:17:38.943938Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ==========================================\n# 1. PROVEN OFFLINE INSTALL (Original Method)\n# ==========================================\nimport os\nimport torch\nimport gc\nimport site\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Cleanup memory\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    gc.collect()\n\nLIB_PATH = \"offline_libs\"\nZIP_PATH = \"/kaggle/input/zno-libs-final/offline_libs.zip\"\n\nif not os.path.exists(LIB_PATH):\n    if os.path.exists(ZIP_PATH):\n        print(\"üì¶ Unzipping original libraries...\")\n        !unzip -q {ZIP_PATH} -d .\n    elif os.path.exists(\"/kaggle/input/zno-libs-final/offline_libs\"):\n        LIB_PATH = \"/kaggle/input/zno-libs-final/offline_libs\"\n\nprint(f\"üì¶ Installing from {LIB_PATH}...\")\n# Reverting to your exact install command\n!pip install --no-index --find-links={LIB_PATH} bitsandbytes peft accelerate transformers > /dev/null\n\nsite.main()\nprint(\"‚úÖ Installation Complete!\")\n\n# ==========================================\n# 2. CONFIGURATION & IMPORTS\n# ==========================================\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\n\nBATCH_SIZE = 1           \nMAX_CONTEXT_LEN = 1500   # Optimized to fit full examples\nBASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1\" \nADAPTER_PATH = \"/kaggle/input/zno-my-adapter\"\n\n# Find test data path\nTEST_FILE_PATH = \"/kaggle/input/zno-data/zno.test.jsonl\"\nif not os.path.exists(TEST_FILE_PATH):\n    for root, _, files in os.walk(\"/kaggle/input\"):\n        if \"zno.test.jsonl\" in files:\n            TEST_FILE_PATH = os.path.join(root, \"zno.test.jsonl\")\n\n# ==========================================\n# 3. LOAD MODEL (Stable 4-bit)\n# ==========================================\nprint(f\"‚è≥ Loading Model (4-bit)...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_PATH,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    local_files_only=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, local_files_only=True)\ntokenizer.padding_side = \"left\" \ntokenizer.pad_token = tokenizer.eos_token \n\nprint(f\"üîó Attaching Adapter...\")\nmodel = PeftModel.from_pretrained(base_model, ADAPTER_PATH, local_files_only=True)\nmodel.eval()\n\n# ==========================================\n# 4. BALANCED LOGIT SCORING (Score Optimizer)\n# ==========================================\n# Mapping Cyrillic letters to model vocabulary\ncandidates = [\"–ê\", \"–ë\", \"–í\", \"–ì\", \"–î\"]\ncandidate_ids = [tokenizer.encode(c, add_special_tokens=False)[-1] for c in candidates]\nanswer_map = {0: \"–ê\", 1: \"–ë\", 2: \"–í\", 3: \"–ì\", 4: \"–î\"}\n\n# Balanced few-shot (Answers are A, B, V, G to eliminate letter bias)\nBALANCED_EXAMPLES = \"\"\"\n–ü–∏—Ç–∞–Ω–Ω—è: –°–ª–æ–≤–æ –∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—é –ø–æ–º–∏–ª–∫–æ—é —î –≤ —Ä—è–¥–∫—É\n–í–∞—Ä—ñ–∞–Ω—Ç–∏:\n–ê: –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∏–π, –±–∞–ª–∞—Å—Ç–Ω–∏–π, —Ñ–æ—Ä–ø–æ—Å—Ç–Ω–∏–π\n–ë: –ø–µ—Å—Ç—É–Ω–∏, —Ö–≤–∞—Å—Ç–ª–∏–≤–∏–π, –∫—ñ—Å—Ç–ª—è–≤–∏–π\n–í: —Å—Ç—É–¥–µ–Ω—Ç—Å—å–∫–∏–π, –¥–∏—Ä–∏–≥–µ–Ω—Ç—Å—å–∫–∏–π, —Ç—É—Ä–∏—Å—Ç—Å—å–∫–∏–π\n–ì: —Ç–∏–∂–Ω–µ–≤–∏–π, —Å–µ—Ä—Ü–µ–≤–∏–π, –∑–ª—ñ—Å–Ω–∏–π\n–î: —É—á–∞—Å–Ω–∏–∫, —è—Ö—Ç—Å–º–µ–Ω, —Å—Ç—ñ–ª—å–Ω–∏–∫–æ–≤–∏–π\n–í—ñ–¥–ø–æ–≤—ñ–¥—å: –ê\n\n–ü–∏—Ç–∞–Ω–Ω—è: –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –≤—Å—ñ —Å–ª–æ–≤–∞ –≤ —Ä—è–¥–∫—É\n–í–∞—Ä—ñ–∞–Ω—Ç–∏:\n–ê: —Å—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ—É–≤–∞—Ç–∏, —Ä–æ–∑–∂—É–≤–∞—Ç–∏, –ø—Ä–∏–ø—Ä—ñ–ª–∏–π\n–ë: –±–µ–∑–∑–∞—Ö–∏—Å–Ω–∏–π, —Å—Ö–∏–±–∏—Ç–∏, —Ä–æ–∑—á–∏–Ω\n–í: –ø–µ—Ä–µ–∫–æ—Ç–∏–ø–æ–ª–µ, –Ω–µ–∑—Ä—ñ–≤–Ω—è–Ω–∏–π, –ø—Ä–∏–ø–∏–Ω–∏—Ç–∏\n–ì: –ø—Ä–µ–≤–µ–ª–µ–±–Ω–∏–π, —Å–∫–∞–∑–∞—Ç–∏, –±–µ–∑—Å–º–µ—Ä—Ç—è\n–î: —Å–ø–∏—Ç–∞—Ç–∏, —Ä–æ–∑—á–µ—Å–∞—Ç–∏, –ø—Ä–∏—Ä–≤–∞\n–í—ñ–¥–ø–æ–≤—ñ–¥—å: –ë\n\n–ü–∏—Ç–∞–Ω–Ω—è: –£–∫–∞–∂—ñ—Ç—å —Ä—è–¥–æ–∫, —É —è–∫–æ–º—É –≤—Å—ñ —Å–ª–æ–≤–∞ –ø–∏—à—É—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏\n–í–∞—Ä—ñ–∞–Ω—Ç–∏:\n–ê: (–®,—à)–µ–≤—á–µ–Ω–∫—ñ–≤—Å—å–∫—ñ –≤—ñ—Ä—à—ñ, (–ö,–∫)–∏—ó–≤—Å—å–∫—ñ –≤—É–ª–∏—Ü—ñ\n–ë: (–î,–¥)–Ω—ñ–ø—Ä–æ–≤—Å—å–∫—ñ —Ö–≤–∏–ª—ñ, (–õ,–ª)—å–≤—ñ–≤—Å—å–∫–∞ –∫–∞–≤–∞\n–í: (–ü,–ø)—ñ–≤–¥–µ–Ω–Ω–∏–π (–ë,–±)—É–≥, (–ó,–∑)–æ–ª–æ—Ç—ñ (–í,–≤)–æ—Ä–æ—Ç–∞\n–ì: (–ù,–Ω)–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π (–ë,–±)–∞–Ω–∫, (–í,–≤)–µ—Ä—Ö–æ–≤–Ω–∞ (–†,—Ä)–∞–¥–∞\n–î: (–ú,–º)—ñ–Ω—ñ—Å—Ç–µ—Ä—Å—Ç–≤–æ (–û,–æ)—Å–≤—ñ—Ç–∏, (–ö,–∫)–∞–±–º—ñ–Ω\n–í—ñ–¥–ø–æ–≤—ñ–¥—å: –í\n\n–ü–∏—Ç–∞–Ω–Ω—è: –£–∫–∞–∂—ñ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ–≥–æ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –ø—Ä–æ–ø—É—Å–∫—ñ–≤\n–í–∞—Ä—ñ–∞–Ω—Ç–∏:\n–ê: –Ω–∞–π–±—ñ–ª—å—à —Ü—ñ–∫–∞–≤—ñ—à–∏–º, –ø–æ —Å–∞–º—ñ –≤—É—Ö–∞, –≤ –∞–Ω—Ñ–∞—Å\n–ë: —è–∫–Ω–∞–π—Ü—ñ–∫–∞–≤—ñ—à–∏–º, –ø–æ –≤—É—Ö–∞, –≤ –∞–Ω—Ñ–∞—Å\n–í: —Ü—ñ–∫–∞–≤—ñ—à–∏–º, –ø–æ —Å–∞–º—ñ –≤—É—Ö–∞, –∞–Ω—Ñ–∞—Å\n–ì: –Ω–∞–π–±—ñ–ª—å—à —Ü—ñ–∫–∞–≤–∏–º, –ø–æ —Å–∞–º—ñ –≤—É—Ö–∞, –≤ –∞–Ω—Ñ–∞—Å\n–î: –Ω–∞–π—Ü—ñ–∫–∞–≤—ñ—à–∏–º, –ø–æ –≤—É—Ö–∞, –∞–Ω—Ñ–∞—Å\n–í—ñ–¥–ø–æ–≤—ñ–¥—å: –ì\n\"\"\"\n\ndef create_prompt(item):\n    q = item.get('question', '')\n    if 'answers' in item:\n        opts = \"\\n\".join([f\"{opt['marker']}: {opt['text']}\" for opt in item['answers']])\n    else:\n        opts = str(item.get('answers', ''))\n    \n    instruction = \"–î–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–∫–≤–æ—é-–≤–∞—Ä—ñ–∞–Ω—Ç–æ–º –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∑ –Ω–∞–¥–∞–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤.\"\n    return f\"<|im_start|>user\\n{instruction}\\n\\n{BALANCED_EXAMPLES}\\n\\n–ü–∏—Ç–∞–Ω–Ω—è: {q}\\n–í–∞—Ä—ñ–∞–Ω—Ç–∏:\\n{opts}<|im_end|>\\n<|im_start|>assistant\\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:\"\n\n# ==========================================\n# 5. EXECUTION\n# ==========================================\ntest_data = []\nwith open(TEST_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        try: test_data.append(json.loads(line))\n        except: pass\n\nprint(f\"üöÄ Starting Optimized Run on {len(test_data)} items...\")\nresults = []\n\nfor i in tqdm(range(0, len(test_data), BATCH_SIZE)):\n    if i % 50 == 0: torch.cuda.empty_cache()\n    \n    batch_items = test_data[i : i + BATCH_SIZE]\n    batch_prompts = [create_prompt(item) for item in batch_items]\n    batch_ids = [item.get('id') for item in batch_items]\n\n    inputs = tokenizer(\n        batch_prompts, \n        return_tensors=\"pt\", \n        padding=True, \n        truncation=True, \n        max_length=MAX_CONTEXT_LEN\n    ).to(\"cuda\")\n\n    with torch.inference_mode(): \n        outputs = model(**inputs)\n        # Higher score: We compare the probability of each answer letter at the decision token\n        next_token_logits = outputs.logits[:, -1, :]\n        candidate_scores = next_token_logits[:, candidate_ids]\n        best_indices = torch.argmax(candidate_scores, dim=1).cpu().numpy()\n\n    for q_id, idx in zip(batch_ids, best_indices):\n        results.append({\"id\": q_id, \"answer\": answer_map[idx]})\n\npd.DataFrame(results).to_csv(\"submission.csv\", index=False)\nprint(f\"‚úÖ Submission created with higher precision strategy!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:17:49.419020Z","iopub.execute_input":"2026-01-13T21:17:49.419316Z","iopub.status.idle":"2026-01-13T21:21:22.598149Z","shell.execute_reply.started":"2026-01-13T21:17:49.419279Z","shell.execute_reply":"2026-01-13T21:21:22.597054Z"}},"outputs":[{"name":"stdout","text":"üì¶ Installing from /kaggle/input/zno-libs-final/offline_libs...\n‚úÖ Installation Complete!\n","output_type":"stream"},{"name":"stderr","text":"2026-01-13 21:18:09.536736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768339089.722283      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768339089.772364      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768339090.198023      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768339090.198061      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768339090.198064      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768339090.198066      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nWARNING:bitsandbytes.cextension:WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\nThis can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\nIf this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n\n","output_type":"stream"},{"name":"stdout","text":"‚è≥ Loading Model (4-bit)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34583179b2844325842cca46c1f15c32"}},"metadata":{}},{"name":"stdout","text":"üîó Attaching Adapter...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'ensure_weight_tying', 'peft_version', 'use_bdlora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Starting Optimized Run on 751 items...\n","output_type":"stream"},{"name":"stderr","text":"  2%|‚ñè         | 12/751 [00:29<29:51,  2.42s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1436148317.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mcandidate_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_token_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mbest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mq_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}