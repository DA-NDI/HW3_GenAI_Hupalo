{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Phase 1: Data Generation (\"The Professor\")"
      ],
      "metadata": {
        "id": "_jCtBpiE1brl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR4cwKnQ1SQL"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. SETUP\n",
        "API_KEY = \"YOUR_GEMINI_API_KEY\"\n",
        "genai.configure(api_key=API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# 2. PROMPT\n",
        "def generate_reasoning(q, options, correct_ans):\n",
        "    prompt = f\"\"\"\n",
        "    You are a strict Ukrainian ZNO exam tutor.\n",
        "    Question: {q}\n",
        "    Options: {options}\n",
        "    Correct Answer: {correct_ans}\n",
        "\n",
        "    Task:\n",
        "    1. Write a concise analysis (1-2 sentences) explaining WHY the correct answer is right and others are wrong.\n",
        "    2. End strictly with the format: \"–í—ñ–¥–ø–æ–≤—ñ–¥—å: [Letter]\"\n",
        "\n",
        "    Output example:\n",
        "    –ê–Ω–∞–ª—ñ–∑: –°–ª–æ–≤–æ \"—Ñ–æ—Ä–ø–æ—Å—Ç–Ω–∏–π\" —î –≤–∏–Ω—è—Ç–∫–æ–º —ñ –ø–∏—à–µ—Ç—å—Å—è –±–µ–∑ –ª—ñ—Ç–µ—Ä–∏ \"—Ç\".\n",
        "    –í—ñ–¥–ø–æ–≤—ñ–¥—å: –ê\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "# 3. GENERATION LOOP\n",
        "input_file = \"zno.train.jsonl\"\n",
        "output_file = \"zno_reasoning_train.jsonl\"\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as fin, \\\n",
        "     open(output_file, 'w', encoding='utf-8') as fout:\n",
        "\n",
        "    for line in tqdm(fin):\n",
        "        item = json.loads(line)\n",
        "\n",
        "        # Prepare options string\n",
        "        opts = \"\\n\".join([f\"{o['marker']}: {o['text']}\" for o in item['answers']])\n",
        "        correct = next(o['marker'] for o in item['answers'] if o['is_correct'])\n",
        "\n",
        "        # Ask Gemini\n",
        "        try:\n",
        "            prompt = generate_reasoning(item['question'], opts, correct)\n",
        "            response = model.generate_content(prompt)\n",
        "\n",
        "            # Save new item with reasoning\n",
        "            new_item = item.copy()\n",
        "            new_item['reasoning_output'] = response.text.strip()\n",
        "            fout.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "            time.sleep(1) # Rate limit safety\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {item['id']}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Phase 2: Fine-Tuning (\"The Student\")\n",
        "Goal: Train Qwen 2.5 7B to mimic Gemini's reasoning using QLoRA. Environment: Kaggle or Colab (GPU T4 x2 or A100)."
      ],
      "metadata": {
        "id": "CedU4xeN1hhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# 1. CONFIG\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "NEW_MODEL_NAME = \"zno-my-adapter\"\n",
        "\n",
        "# 2. LOAD MODEL (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 3. PREPARE DATASET\n",
        "def format_prompt(sample):\n",
        "    # Format: User Prompt -> Gemini Reasoning Output\n",
        "    q = sample['question']\n",
        "    opts = \"\\n\".join([f\"{o['marker']}: {o['text']}\" for o in sample['answers']])\n",
        "\n",
        "    user_msg = f\"<|im_start|>user\\n{q}\\n{opts}<|im_end|>\\n\"\n",
        "    assist_msg = f\"<|im_start|>assistant\\n{sample['reasoning_output']}<|im_end|>\"\n",
        "\n",
        "    return {\"text\": user_msg + assist_msg}\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"zno_reasoning_train.jsonl\", split=\"train\")\n",
        "dataset = dataset.map(format_prompt)\n",
        "\n",
        "# 4. LoRA CONFIG\n",
        "peft_config = LoraConfig(\n",
        "    r=16, lora_alpha=16, lora_dropout=0.05,\n",
        "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "# 5. TRAIN\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=200, # Adjust based on data size (e.g., 1 epoch)\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    peft_config=peft_config\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(NEW_MODEL_NAME)\n",
        "print(\"‚úÖ Adapter Saved!\")"
      ],
      "metadata": {
        "id": "q9GJMfOh1g41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Phase 3: Offline Preparation\n",
        "Goal: Download libraries to install them without internet during the exam. Environment: Local Machine (with Internet)."
      ],
      "metadata": {
        "id": "gLTJvREG1uC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create folder\n",
        "mkdir offline_libs\n",
        "\n",
        "# Download wheels (ignoring dependencies to save space/conflicts)\n",
        "pip download -d offline_libs --no-deps bitsandbytes peft accelerate transformers tokenizers safetensors sentencepiece\n",
        "\n",
        "# Zip it\n",
        "zip -r offline_libs.zip offline_libs"
      ],
      "metadata": {
        "id": "z5zUJyl_1wyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload offline_libs.zip as a Dataset to Kaggle."
      ],
      "metadata": {
        "id": "tkyrqEgu1030"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Phase 4: Final Inference (\"The Exam\")\n",
        "\n",
        "Goal: Offline inference using Logit Scoring (highest accuracy) and the Paper-Optimized Prompt. Environment: Kaggle Notebook (Offline, GPU P100). Config: Batch Size 1 (Stability), Logits Only (No Generation)."
      ],
      "metadata": {
        "id": "a4ZZzeB114oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. SETUP & INSTALL\n",
        "# ==========================================\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Clean memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Install libs offline\n",
        "LIB_PATH = \"offline_libs\"\n",
        "ZIP_PATH = \"/kaggle/input/zno-libs-final/offline_libs.zip\"\n",
        "\n",
        "if not os.path.exists(LIB_PATH) and os.path.exists(ZIP_PATH):\n",
        "    print(\"üì¶ Unzipping libraries...\")\n",
        "    !unzip -q {ZIP_PATH} -d .\n",
        "\n",
        "print(f\"üì¶ Installing libraries...\")\n",
        "!pip install --no-index --find-links={LIB_PATH} bitsandbytes peft accelerate transformers tokenizers safetensors sentencepiece > /dev/null\n",
        "print(\"‚úÖ Done!\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. CONFIGURATION (STABLE + HIGH SCORE)\n",
        "# ==========================================\n",
        "BATCH_SIZE = 1           # Safest for P100\n",
        "MAX_CONTEXT_LEN = 1100   # Sufficient for ZNO questions\n",
        "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1\"\n",
        "ADAPTER_PATH = \"/kaggle/input/zno-my-adapter\"\n",
        "TEST_FILE_PATH = \"/kaggle/input/zno-data/zno.test.jsonl\"\n",
        "\n",
        "# ==========================================\n",
        "# 3. LOAD MODEL\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "if not torch.cuda.is_available(): raise SystemError(\"‚ùå Turn on GPU!\")\n",
        "\n",
        "print(f\"‚è≥ Loading Model...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_PATH, quantization_config=bnb_config, device_map=\"auto\", local_files_only=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, local_files_only=True)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH, local_files_only=True)\n",
        "model.eval()\n",
        "\n",
        "# ==========================================\n",
        "# 4. LOGIT STRATEGY (Paper Optimized)\n",
        "# ==========================================\n",
        "# Mapping Cyrillic candidates [cite: 87]\n",
        "candidates = [\"–ê\", \"–ë\", \"–í\", \"–ì\", \"–î\"]\n",
        "candidate_ids = [tokenizer.encode(c, add_special_tokens=False)[-1] for c in candidates]\n",
        "answer_map = {0: \"–ê\", 1: \"–ë\", 2: \"–í\", 3: \"–ì\", 4: \"–î\"}\n",
        "\n",
        "# Few-shot examples and strict prompt [cite: 91, 97]\n",
        "EXAMPLES = \"\"\"\n",
        "–ü–∏—Ç–∞–Ω–Ω—è: –°–ª–æ–≤–æ –∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—é –ø–æ–º–∏–ª–∫–æ—é —î –≤ —Ä—è–¥–∫—É\n",
        "–í–∞—Ä—ñ–∞–Ω—Ç–∏:\n",
        "–ê: –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∏–π, –±–∞–ª–∞—Å—Ç–Ω–∏–π, —Ñ–æ—Ä–ø–æ—Å—Ç–Ω–∏–π\n",
        "–ë: –ø–µ—Å—Ç—É–Ω–∏, —Ö–≤–∞—Å—Ç–ª–∏–≤–∏–π, –∫—ñ—Å—Ç–ª—è–≤–∏–π\n",
        "–í: —Å—Ç—É–¥–µ–Ω—Ç—Å—å–∫–∏–π, –¥–∏—Ä–∏–≥–µ–Ω—Ç—Å—å–∫–∏–π, —Ç—É—Ä–∏—Å—Ç—Å—å–∫–∏–π\n",
        "–ì: —Ç–∏–∂–Ω–µ–≤–∏–π, —Å–µ—Ä—Ü–µ–≤–∏–π, –∑–ª—ñ—Å–Ω–∏–π\n",
        "–î: —É—á–∞—Å–Ω–∏–∫, —è—Ö—Ç—Å–º–µ–Ω, —Å—Ç—ñ–ª—å–Ω–∏–∫–æ–≤–∏–π\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å: –ê\n",
        "\n",
        "–ü–∏—Ç–∞–Ω–Ω—è: –£–∫–∞–∂—ñ—Ç—å —Ä—è–¥–æ–∫, —É —è–∫–æ–º—É –≤—Å—ñ —Å–ª–æ–≤–∞ –ø–∏—à—É—Ç—å—Å—è –∑ –≤–µ–ª–∏–∫–æ—ó –ª—ñ—Ç–µ—Ä–∏\n",
        "–í–∞—Ä—ñ–∞–Ω—Ç–∏:\n",
        "–ê: (–®,—à)–µ–≤—á–µ–Ω–∫—ñ–≤—Å—å–∫—ñ –≤—ñ—Ä—à—ñ, (–ö,–∫)–∏—ó–≤—Å—å–∫—ñ –≤—É–ª–∏—Ü—ñ\n",
        "–ë: (–î,–¥)–Ω—ñ–ø—Ä–æ–≤—Å—å–∫—ñ —Ö–≤–∏–ª—ñ, (–õ,–ª)—å–≤—ñ–≤—Å—å–∫–∞ –∫–∞–≤–∞\n",
        "–í: (–ü,–ø)—ñ–≤–¥–µ–Ω–Ω–∏–π (–ë,–±)—É–≥, (–ó,–∑)–æ–ª–æ—Ç—ñ (–í,–≤)–æ—Ä–æ—Ç–∞\n",
        "–ì: (–ù,–Ω)–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π (–ë,–±)–∞–Ω–∫, (–í,–≤)–µ—Ä—Ö–æ–≤–Ω–∞ (–†,—Ä)–∞–¥–∞\n",
        "–î: (–ú,–º)—ñ–Ω—ñ—Å—Ç–µ—Ä—Å—Ç–≤–æ (–û,–æ)—Å–≤—ñ—Ç–∏, (–ö,–∫)–∞–±–º—ñ–Ω\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å: –í\n",
        "\"\"\"\n",
        "\n",
        "def create_prompt(item):\n",
        "    q = item.get('question', '')\n",
        "    opts = \"\\n\".join([f\"{o['marker']}: {o['text']}\" for o in item.get('answers', [])]) if 'answers' in item else str(item.get('answers', ''))\n",
        "\n",
        "    # Paper-optimized instruction\n",
        "    instruction = \"–î–∞–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–∫–≤–æ—é-–≤–∞—Ä—ñ–∞–Ω—Ç–æ–º –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∑ –Ω–∞–¥–∞–Ω–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤.\"\n",
        "    return f\"<|im_start|>user\\n{instruction}\\n\\n–ü—Ä–∏–∫–ª–∞–¥–∏:\\n{EXAMPLES}\\n\\n–ü–∏—Ç–∞–Ω–Ω—è: {q}\\n–í–∞—Ä—ñ–∞–Ω—Ç–∏:\\n{opts}<|im_end|>\\n<|im_start|>assistant\\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:\"\n",
        "\n",
        "# ==========================================\n",
        "# 5. EXECUTION LOOP\n",
        "# ==========================================\n",
        "if not os.path.exists(TEST_FILE_PATH):\n",
        "    for root, _, files in os.walk(\"/kaggle/input\"):\n",
        "        if \"zno.test.jsonl\" in files: TEST_FILE_PATH = os.path.join(root, \"zno.test.jsonl\")\n",
        "\n",
        "test_data = []\n",
        "with open(TEST_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try: test_data.append(json.loads(line))\n",
        "        except: pass\n",
        "\n",
        "print(f\"üöÄ Starting Inference on {len(test_data)} items...\")\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(0, len(test_data), BATCH_SIZE)):\n",
        "    if i % 50 == 0: torch.cuda.empty_cache() # Keep memory clean\n",
        "\n",
        "    batch = test_data[i : i + BATCH_SIZE]\n",
        "    prompts = [create_prompt(item) for item in batch]\n",
        "    ids = [item['id'] for item in batch]\n",
        "\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_CONTEXT_LEN).to(\"cuda\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model(**inputs)\n",
        "        # Logit Scoring: Check probability of A, B, C, D, E at the last token position\n",
        "        logits = outputs.logits[:, -1, candidate_ids]\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "    for q_id, idx in zip(ids, preds):\n",
        "        results.append({\"id\": q_id, \"answer\": answer_map[idx]})\n",
        "\n",
        "pd.DataFrame(results).to_csv(\"submission.csv\", index=False)\n",
        "print(f\"‚úÖ Submission Saved!\")"
      ],
      "metadata": {
        "id": "jJ9d5rCx1qPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}